{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":956177,"sourceType":"datasetVersion","datasetId":519884},{"sourceId":6945939,"sourceType":"datasetVersion","datasetId":3989137},{"sourceId":7615428,"sourceType":"datasetVersion","datasetId":4434986},{"sourceId":9146200,"sourceType":"datasetVersion","datasetId":5524489},{"sourceId":9971597,"sourceType":"datasetVersion","datasetId":6134770},{"sourceId":214408771,"sourceType":"kernelVersion"},{"sourceId":207745,"sourceType":"modelInstanceVersion","modelInstanceId":177104,"modelId":199398},{"sourceId":207992,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":177322,"modelId":199627}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Dataset paths\nREAL_PATH = \"/kaggle/input/faceforensics/FF++/real\"\nFAKE_PATH = \"/kaggle/input/faceforensics/FF++/fake\"\nOUTPUT_FRAME_SIZE = (128, 128)  # Frame dimensions\nFRAME_COUNT = 10  # Number of frames to extract per video\nMAX_VIDEOS = 700  # Number of videos to process from each category\n\n# Function to extract frames from a video\ndef extract_frames(video_path, output_size=(128, 128), frame_count=10):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    step = max(total_frames // frame_count, 1)  # Uniform sampling\n    \n    for i in range(frame_count):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.resize(frame, output_size)\n        frames.append(frame)\n    cap.release()\n    return np.array(frames)\n\n# Prepare data and labels\ndata = []\nlabels = []\n\n# Process real videos\nprint(\"Processing real videos...\")\nreal_videos = os.listdir(REAL_PATH)[:MAX_VIDEOS]   # Limit to 300 videos\nfor video_file in tqdm(real_videos):\n    video_path = os.path.join(REAL_PATH, video_file)\n    frames = extract_frames(video_path, output_size=OUTPUT_FRAME_SIZE, frame_count=FRAME_COUNT)\n    if len(frames) == FRAME_COUNT:  # Ensure correct frame count\n        data.append(frames)\n        labels.append(0)  # Label 0 for real\n\n# Process fake videos\nprint(\"Processing fake videos...\")\nfake_videos = os.listdir(FAKE_PATH)[:MAX_VIDEOS]  # Limit to 300 videos\nfor video_file in tqdm(fake_videos):\n    video_path = os.path.join(FAKE_PATH, video_file)\n    frames = extract_frames(video_path, output_size=OUTPUT_FRAME_SIZE, frame_count=FRAME_COUNT)\n    if len(frames) == FRAME_COUNT:\n        data.append(frames)\n        labels.append(1)  # Label 1 for fake\n\n# Convert to numpy arrays\ndata = np.array(data)  # Shape: (num_videos, num_frames, 128, 128, 3)\nlabels = np.array(labels)\n\n# Split into train, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Normalize data\nX_train = X_train / 255.0\nX_val = X_val / 255.0\nX_test = X_test / 255.0\n\n# Convert labels to categorical\ny_train = to_categorical(y_train, num_classes=2)\ny_val = to_categorical(y_val, num_classes=2)\ny_test = to_categorical(y_test, num_classes=2)\n\nprint(f\"Data shapes: Train - {X_train.shape}, Validation - {X_val.shape}, Test - {X_test.shape}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-02-09T17:41:59.419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Augment frames\ndatagen = ImageDataGenerator(\n    horizontal_flip=True,\n    rotation_range=10,\n    zoom_range=0.1,\n    brightness_range=[0.8, 1.2]\n)\n\n# Function to augment extracted frames\ndef augment_frames(frames):\n    augmented_frames = []\n    for frame in frames:\n        frame = datagen.random_transform(frame)\n        augmented_frames.append(frame)\n    return np.array(augmented_frames)\n\n# Augment training data\naugmented_data = []\naugmented_labels = []\n\nfor i in range(len(X_train)):\n    augmented_frames = augment_frames(X_train[i])\n    augmented_data.append(augmented_frames)\n    augmented_labels.append(y_train[i])\n\n# Combine original and augmented data\nX_train_augmented = np.concatenate((X_train, np.array(augmented_data)))\ny_train_augmented = np.concatenate((y_train, np.array(augmented_labels)))\n\nprint(f\"Augmented Train Data: {X_train_augmented.shape}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-09T17:36:34.024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.layers import Dense, Flatten, TimeDistributed, LSTM\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout\n\n# Define model\ndef build_improved_model(input_shape=(FRAME_COUNT, 128, 128, 3)):\n    model = Sequential([\n        TimeDistributed(Xception(weights='imagenet', include_top=False, input_shape=(128, 128, 3))),\n        TimeDistributed(Flatten()),\n        Dropout(0.5),  # Add dropout for regularization\n        LSTM(128, return_sequences=False),\n        Dropout(0.5),  # Add dropout\n        Dense(64, activation='relu'),\n        Dense(2, activation='softmax')\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\nmodel = build_improved_model()\nmodel.summary()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-09T17:36:34.024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\n# Model checkpoint to save the best model in .keras format\ncheckpoint = ModelCheckpoint(\n    \"deepfake_detection_model.keras\",  # Change to .keras\n    monitor=\"val_accuracy\",\n    save_best_only=True,\n    verbose=1\n)\n\n# Reduce learning rate on plateau\nlr_scheduler = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=3,\n    verbose=1\n)\n\n# Train the model\nhistory = model.fit(\n    X_train_augmented, y_train_augmented,\n    validation_data=(X_val, y_val),\n    epochs=40,\n    batch_size=10,\n    callbacks=[checkpoint, lr_scheduler]\n)\nmodel.save(\"deepfake_detection_model.keras\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-09T17:36:34.024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Testing","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score\n\n# Load the best saved model\nfrom tensorflow.keras.models import load_model\nmodel = load_model('deepfake_detection_model.keras')\n\n# Evaluate on test set\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n# Metrics\naccuracy = accuracy_score(y_true, y_pred_classes)\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n\n# Precision, Recall, F1-Score\nprint(\"Classification Report:\")\nprint(classification_report(y_true, y_pred_classes, target_names=['REAL', 'FAKE']))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-09T17:36:34.024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\nimport numpy as np\n\n# Plot accuracy and loss graphs\ndef plot_training_history(history):\n    # Accuracy\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Train Accuracy')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    # Loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Train Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title('Model Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n# Compute confusion matrix\ndef plot_confusion_matrix(model, X_test, y_test):\n    # Get predictions\n    y_pred = model.predict(X_test)\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true = np.argmax(y_test, axis=1)\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred_classes)\n    cm_labels = ['Real', 'Fake']\n\n    # Plot confusion matrix\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cm_labels, yticklabels=cm_labels)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.show()\n\n    # Classification report\n    print(classification_report(y_true, y_pred_classes, target_names=cm_labels))\n\n# Plot training history\nplot_training_history(history)\n\n# Plot confusion matrix\nplot_confusion_matrix(model, X_test, y_test)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-09T17:36:34.024Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Real Time Detection","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-09T17:36:34.024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to extract frames from a video\ndef extract_frames(video_path, output_size=(128, 128), frame_count=10):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    step = max(total_frames // frame_count, 1)  # Uniform sampling\n    \n    for i in range(frame_count):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame = cv2.resize(frame, output_size)\n        frames.append(frame)\n    cap.release()\n    return np.array(frames)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-09T17:36:34.024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the model for real-time detection\nloaded_model = load_model('deepfake_detection_model.keras')\n\ndef predict_video(video_path, model, output_size=(128, 128), frame_count=10):\n    frames = extract_frames(video_path, output_size, frame_count)\n    frames = frames / 255.0  # Normalize\n    frames = np.expand_dims(frames, axis=0)  # Add batch dimension\n    prediction = model.predict(frames)\n    label = \"FAKE\" if np.argmax(prediction) == 1 else \"REAL\"\n    confidence = prediction[0][np.argmax(prediction)]\n    print(f\"Prediction: {label} (Confidence: {confidence:.2f})\")\n\nREAL_PATH = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences\"\nFAKE_PATH = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences\"\n# Test prediction on a video\nreal_sample_path = os.path.join(REAL_PATH, \"/kaggle/input/deepfake-testing-videos/model1.mp4\")  # Replace with real video path\nfake_sample_path = os.path.join(FAKE_PATH, \"/kaggle/input/deepfake-testing-videos/modeloutput1.mp4\")  # Replace with fake video path\n\nprint(\"Real Video Prediction:\")\npredict_video(real_sample_path,loaded_model)\n\nprint(\"Fake Video Prediction:\")\npredict_video(fake_sample_path,loaded_model)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-09T17:36:34.024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}